Complete Architecture Plan: Sui Indexer â†’ Redis Pub/Sub â†’ SSE Server â†’ Frontend
Overview
We're building a real-time event streaming system for DID (Decentralized Identity) verification events on Sui blockchain.
Architecture Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sui Indexer       â”‚ (Rust - processes blockchain checkpoints)
â”‚   (Existing)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€> PostgreSQL (permanent storage for all events)
           â”‚         - Table: did_claimed_events
           â”‚         - Used for: historical queries, user dashboards
           â”‚
           â””â”€â”€â”€â”€â”€â”€> Redis Pub/Sub (ephemeral messaging only)
                    - Channel: "did_claimed"
                    - NO STORAGE (Pub/Sub is fire-and-forget)
                    - If no subscriber exists, message is lost (acceptable)
                         â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  SSE API Server    â”‚ (New - to be built)
                â”‚  (Rust + Axum)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”œâ”€â”€â”€â”€â”€â”€> SSE Endpoint: /api/sse/events
                         â”‚         (subscribes to Redis, pushes to clients)
                         â”‚
                         â””â”€â”€â”€â”€â”€â”€> REST API: /api/events
                                   (queries PostgreSQL for historical data)
                                        â†“
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚   Frontend    â”‚
                                â”‚ (React/Next)  â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Current State (What We Have)
1. Sui Indexer (suiverify-indexer/)

âœ… Processes Sui checkpoints from Testnet
âœ… Extracts DIDClaimed events using BCS deserialization
âœ… Stores events in PostgreSQL table did_claimed_events
âœ… Working and running

Files:

src/handlers.rs - Transaction digest handler
src/event_handlers.rs - DIDClaimed event handler
src/models.rs - Database models
src/events.rs - Event struct definitions
src/schema.rs - Database schema

2. PostgreSQL Database

âœ… Table: did_claimed_events with columns:

id, registry_id, user_address, did_type, user_did_id, nft_id
checkpoint_sequence_number, transaction_digest, timestamp_ms, event_index


âœ… Contains indexed DID claim events

3. WebSocket Server (did-explorer/)

âŒ Currently trying to use PostgreSQL LISTEN/NOTIFY (not working)
âŒ Has lifetime/async issues with tokio-postgres
ğŸ”„ NEEDS TO BE REPLACED with Redis-based solution


Required Changes
Change 1: Update Sui Indexer to Publish to Redis
File: suiverify-indexer/src/event_handlers.rs
Current code:
rustasync fn commit<'a>(
    &self,
    batch: &Self::Batch,
    conn: &mut <Self::Store as Store>::Connection<'a>,
) -> Result<usize> {
    // Only writes to PostgreSQL
    let inserted = diesel::insert_into(did_claimed_events)
        .values(batch)
        .on_conflict((transaction_digest, event_index))
        .do_nothing()
        .execute(conn)
        .await?;

    Ok(inserted)
}
New code needed:
rustasync fn commit<'a>(
    &self,
    batch: &Self::Batch,
    conn: &mut <Self::Store as Store>::Connection<'a>,
) -> Result<usize> {
    // 1. Write to PostgreSQL (permanent storage)
    let inserted = diesel::insert_into(did_claimed_events)
        .values(batch)
        .on_conflict((transaction_digest, event_index))
        .do_nothing()
        .execute(conn)
        .await?;

    // 2. Publish to Redis Pub/Sub (real-time notifications)
    // Note: If no subscribers, message is lost - this is acceptable
    if let Ok(redis_client) = redis::Client::open(std::env::var("REDIS_URL").unwrap_or_default()) {
        if let Ok(mut redis_con) = redis_client.get_multiplexed_async_connection().await {
            for event in batch {
                let event_json = serde_json::to_string(&StoredDIDClaimedEvent {
                    registry_id: event.registry_id.clone(),
                    user_address: event.user_address.clone(),
                    did_type: event.did_type,
                    user_did_id: event.user_did_id.clone(),
                    nft_id: event.nft_id.clone(),
                    checkpoint_sequence_number: event.checkpoint_sequence_number,
                    transaction_digest: event.transaction_digest.clone(),
                    timestamp_ms: event.timestamp_ms,
                    event_index: event.event_index,
                })?;
                
                // PUBLISH to Redis Pub/Sub channel
                let _: () = redis_con.publish("did_claimed", event_json).await
                    .unwrap_or_else(|e| {
                        // Log but don't fail if Redis publish fails
                        eprintln!("âš ï¸  Failed to publish to Redis: {}", e);
                    });
                
                info!("ğŸ“¤ Published event to Redis Pub/Sub");
            }
        } else {
            warn!("âš ï¸  No Redis connection - event not published (subscribers will miss this)");
        }
    }

    Ok(inserted)
}
Dependencies to add in suiverify-indexer/Cargo.toml:
toml[dependencies]
# Existing dependencies...
redis = { version = "0.24", features = ["tokio-comp", "connection-manager"] }
Environment variable in suiverify-indexer/.env:
bashDATABASE_URL=postgresql://ash-win@localhost:5432/sui_indexer
REDIS_URL=redis://default:YOUR_PASSWORD@redis-11134.crce182.ap-south-1-1.ec2.cloud.redislabs.com:11134

Change 2: Build New SSE API Server (Replace WebSocket Server)
Project: did-explorer/ (rename from websocket-server)
Delete these files:

src/db_listener.rs (we won't use PostgreSQL LISTEN/NOTIFY)
src/websocket_handler.rs (we won't use WebSocket)

Create new files:
File: src/redis_subscriber.rs
rustuse redis::AsyncCommands;
use tokio::sync::broadcast;
use anyhow::Result;
use tracing::{info, error, warn};

use crate::types::DIDClaimedEvent;

pub struct RedisSubscriber {
    tx: broadcast::Sender<DIDClaimedEvent>,
}

impl RedisSubscriber {
    pub fn new(tx: broadcast::Sender<DIDClaimedEvent>) -> Self {
        Self { tx }
    }

    pub async fn start(&self, redis_url: String) -> Result<()> {
        let tx_clone = self.tx.clone();
        
        tokio::spawn(async move {
            info!("ğŸ”Œ Connecting to Redis...");
            
            let client = redis::Client::open(redis_url).unwrap();
            let mut pubsub = client.get_async_pubsub().await.unwrap();
            
            // SUBSCRIBE to the channel
            pubsub.subscribe("did_claimed").await.unwrap();
            info!("âœ… Subscribed to Redis channel: did_claimed");
            
            let mut stream = pubsub.on_message();
            
            info!("ğŸ‘‚ Listening for DID claim events from Redis...");
            
            // Listen for messages
            while let Some(msg) = stream.next().await {
                let payload: String = msg.get_payload().unwrap();
                
                info!("ğŸ“¬ Received event from Redis Pub/Sub");
                
                match serde_json::from_str::<DIDClaimedEvent>(&payload) {
                    Ok(event) => {
                        info!("ğŸ“¤ Broadcasting to {} SSE clients", tx_clone.receiver_count());
                        
                        match tx_clone.send(event) {
                            Ok(n) => info!("âœ… Pushed to {} client(s)", n),
                            Err(_) => warn!("âš ï¸  No SSE clients connected"),
                        }
                    }
                    Err(e) => error!("âŒ Failed to parse event: {}", e),
                }
            }
        });
        
        Ok(())
    }
}
File: src/api_handlers.rs
rustuse axum::{
    extract::{State, Query},
    response::sse::{Event, Sse},
    Json,
};
use futures_util::stream::Stream;
use std::convert::Infallible;
use tokio::sync::broadcast;
use serde::Deserialize;
use sqlx::PgPool;

use crate::types::DIDClaimedEvent;

// SSE endpoint - for real-time push from Redis
pub async fn sse_events(
    State(rx): State<broadcast::Sender<DIDClaimedEvent>>,
) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {
    let mut receiver = rx.subscribe();
    
    let stream = async_stream::stream! {
        info!("ğŸ”— New SSE client connected");
        
        while let Ok(event) = receiver.recv().await {
            let json = serde_json::to_string(&event).unwrap();
            yield Ok(Event::default().data(json));
        }
        
        info!("ğŸ‘‹ SSE client disconnected");
    };
    
    Sse::new(stream).keep_alive(
        axum::response::sse::KeepAlive::new()
            .interval(std::time::Duration::from_secs(15))
            .text("keepalive")
    )
}

// REST endpoint - for historical queries from PostgreSQL
#[derive(Deserialize)]
pub struct EventQuery {
    pub limit: Option<i64>,
    pub user_address: Option<String>,
}

pub async fn get_recent_events(
    State(db): State<PgPool>,
    Query(params): Query<EventQuery>,
) -> Json<Vec<DIDClaimedEvent>> {
    let limit = params.limit.unwrap_or(20);
    
    let query = if let Some(user_addr) = params.user_address {
        sqlx::query_as!(
            DIDClaimedEvent,
            r#"
            SELECT 
                registry_id, user_address, did_type, user_did_id, nft_id,
                checkpoint_sequence_number as "checkpoint",
                transaction_digest as "tx_digest",
                timestamp_ms as "timestamp"
            FROM did_claimed_events
            WHERE user_address = $1
            ORDER BY timestamp_ms DESC
            LIMIT $2
            "#,
            user_addr,
            limit
        )
    } else {
        sqlx::query_as!(
            DIDClaimedEvent,
            r#"
            SELECT 
                registry_id, user_address, did_type, user_did_id, nft_id,
                checkpoint_sequence_number as "checkpoint",
                transaction_digest as "tx_digest",
                timestamp_ms as "timestamp"
            FROM did_claimed_events
            ORDER BY timestamp_ms DESC
            LIMIT $1
            "#,
            limit
        )
    };
    
    let events = query.fetch_all(&db).await.unwrap_or_default();
    
    Json(events)
}
File: src/main.rs
rustmod types;
mod redis_subscriber;
mod api_handlers;

use axum::{
    routing::get,
    Router,
};
use tokio::sync::broadcast;
use sqlx::postgres::PgPoolOptions;
use tracing::info;
use anyhow::Result;
use std::net::SocketAddr;

use crate::types::DIDClaimedEvent;
use crate::redis_subscriber::RedisSubscriber;
use crate::api_handlers::{sse_events, get_recent_events};

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging
    tracing_subscriber::fmt::init();
    
    // Load environment variables
    dotenvy::dotenv().ok();
    
    let database_url = std::env::var("DATABASE_URL")
        .expect("DATABASE_URL must be set");
    let redis_url = std::env::var("REDIS_URL")
        .expect("REDIS_URL must be set");
    
    info!("ğŸš€ Starting SSE API Server");
    
    // Create PostgreSQL connection pool for REST queries
    let db_pool = PgPoolOptions::new()
        .max_connections(5)
        .connect(&database_url)
        .await?;
    
    info!("âœ… Connected to PostgreSQL");
    
    // Create broadcast channel for SSE
    let (tx, _rx) = broadcast::channel::<DIDClaimedEvent>(100);
    
    // Start Redis subscriber
    let subscriber = RedisSubscriber::new(tx.clone());
    tokio::spawn(async move {
        if let Err(e) = subscriber.start(redis_url).await {
            error!("âŒ Redis subscriber error: {}", e);
        }
    });
    
    // Build API routes
    let app = Router::new()
        // SSE endpoint for real-time updates
        .route("/api/sse/events", get(sse_events))
        // REST endpoint for historical queries
        .route("/api/events", get(get_recent_events))
        .with_state(tx)  // For SSE
        .with_state(db_pool);  // For REST
    
    // Start server
    let addr = SocketAddr::from(([127, 0, 0, 1], 8080));
    info!("âœ… API server listening on http://{}", addr);
    info!("ğŸ“¡ SSE endpoint: http://{}/api/sse/events", addr);
    info!("ğŸ” REST endpoint: http://{}/api/events", addr);
    
    axum::Server::bind(&addr)
        .serve(app.into_make_service())
        .await?;
    
    Ok(())
}
File: src/types.rs (update for sqlx)
rustuse serde::{Deserialize, Serialize};
use sqlx::FromRow;

/// Event sent to SSE clients
#[derive(Debug, Clone, Serialize, Deserialize, FromRow)]
pub struct DIDClaimedEvent {
    pub registry_id: String,
    pub user_address: String,
    pub did_type: i16,
    pub user_did_id: String,
    pub nft_id: String,
    pub checkpoint: i64,
    pub tx_digest: String,
    pub timestamp: i64,
}
Update did-explorer/Cargo.toml:
toml[package]
name = "sse-api-server"
version = "0.1.0"
edition = "2021"

[dependencies]
# Web framework
axum = "0.7"
tower = "0.4"
tower-http = { version = "0.5", features = ["cors"] }

# Async runtime
tokio = { version = "1.0", features = ["full"] }

# Redis (Pub/Sub only, NO streams)
redis = { version = "0.24", features = ["tokio-comp", "connection-manager", "streams"] }

# PostgreSQL for queries
sqlx = { version = "0.7", features = ["runtime-tokio-native-tls", "postgres", "chrono"] }

# Async streaming
async-stream = "0.3"
futures-util = "0.3"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Error handling
anyhow = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = "0.3"

# Environment
dotenvy = "0.15"
```

---

## Redis Pub/Sub Behavior (Important!)

### What Happens When Events Are Published:

**Scenario 1: Subscriber exists (SSE server running)**
```
Indexer â†’ PUBLISH to Redis â†’ Redis sends to subscriber â†’ SSE pushes to clients âœ…
```

**Scenario 2: No subscriber (SSE server not running)**
```
Indexer â†’ PUBLISH to Redis â†’ Redis drops message (no one listening) âš ï¸
This is acceptable because:

Event is still in PostgreSQL (permanent record)
When SSE server starts, clients can fetch from PostgreSQL via REST API
Only real-time push is missed, not the data itself

Success Logging in Indexer:
rust// In indexer after Redis publish
info!("âœ… Event written to PostgreSQL (permanent)");
info!("ğŸ“¤ Event published to Redis Pub/Sub (subscribers: {})", 
     redis_con.pubsub_numsub("did_claimed").await.unwrap_or(0));
This shows if anyone is listening.

Frontend Integration
On Page Load:
javascript// Fetch historical events from PostgreSQL
const response = await fetch('/api/events?limit=20');
const events = await response.json();
setEvents(events);
For Live Updates:
javascript// Subscribe to SSE
const eventSource = new EventSource('/api/sse/events');

eventSource.onmessage = (e) => {
    const newEvent = JSON.parse(e.data);
    setEvents(prev => [newEvent, ...prev]);
};
When User Clicks Event:
javascript// Fetch details from PostgreSQL if needed
const response = await fetch(`/api/events/${event.nft_id}`);
const details = await response.json();
```

---

## Data Flow for One Event
```
ğŸ“Š Event 1: DIDClaimed happens on blockchain
     â†“
ğŸ” Indexer processes checkpoint 264113881
     â†“
ğŸ’¾ WRITE to PostgreSQL
   âœ… Event stored permanently
     â†“
ğŸ“¡ PUBLISH to Redis "did_claimed" channel
   â”œâ”€> If SSE server subscribed: âœ… Message delivered
   â””â”€> If no subscribers: âš ï¸  Message dropped (acceptable - data in PostgreSQL)
     â†“
ğŸŒ SSE Server receives from Redis
     â†“
ğŸ“¤ SSE pushes to all connected frontend clients
     â†“
ğŸ–¥ï¸  Frontend UI updates instantly
     â†“
ğŸ”„ User refreshes page
     â†“
ğŸ“ Frontend calls GET /api/events
     â†“
ğŸ’¾ Queries PostgreSQL
     â†“
ğŸ–¥ï¸  Shows all events including Event 1

Key Points
Redis Pub/Sub Characteristics:

NO STORAGE - Messages are not persisted
Fire-and-forget - If no subscriber, message is lost
Instant delivery - <1ms latency when subscriber exists
Lightweight - Minimal memory usage

Why This Is OK:

âœ… PostgreSQL has all data (source of truth)
âœ… Real-time is bonus, not requirement
âœ… Clients can always query PostgreSQL
âœ… Simpler than Redis Streams (no retention management)

What We DON'T Use:

âŒ Redis Streams (you said no streams - correct!)
âŒ WebSocket (SSE is simpler for one-way push)
âŒ PostgreSQL LISTEN/NOTIFY (complex in async Rust)


Testing Plan
1. Test Indexer â†’ Redis:
bash# Terminal 1: Monitor Redis
redis-cli -h redis-11134.crce182.ap-south-1-1.ec2.cloud.redislabs.com \
          -p 11134 \
          -a YOUR_PASSWORD \
          SUBSCRIBE did_claimed

# Terminal 2: Run indexer
cd suiverify-indexer
cargo run --release -- --remote-store-url https://checkpoints.testnet.sui.io --first-checkpoint 264113000

# You should see messages appear in Terminal 1 when events are indexed
2. Test SSE Server:
bash# Terminal 1: Run SSE server
cd did-explorer
cargo run

# Terminal 2: Test SSE endpoint
curl -N http://localhost:8080/api/sse/events

# Terminal 3: Trigger event (manual Redis publish)
redis-cli -h ... -a ... PUBLISH did_claimed '{"registry_id":"0xTEST",...}'

# Terminal 2 should receive the event via SSE
3. Test REST API:
bashcurl http://localhost:8080/api/events
# Should return JSON array of events from PostgreSQL

Summary for AI Agent
Build an SSE (Server-Sent Events) API server that:

Subscribes to Redis Pub/Sub channel did_claimed
Receives DIDClaimedEvent JSON from Redis (published by indexer)
Broadcasts to all connected SSE clients via /api/sse/events endpoint
Queries PostgreSQL for historical events via /api/events REST endpoint

Tech stack:

Axum (web framework)
Redis (async pub/sub client)
sqlx (PostgreSQL queries)
SSE (server-sent events for real-time push)

Do NOT use:

WebSocket (overkill)
Redis Streams (not needed)
PostgreSQL LISTEN/NOTIFY (complex)